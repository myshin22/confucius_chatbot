{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Follow steps along numbers</h4>\n",
    "\n",
    "#1 Variables <Br>\n",
    "#2 Style init<Br>\n",
    "#3 Context init (Three options)<Br>\n",
    "#4 Main<Br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Variables\n",
    "MODEL_NAME = \"gpt-3.5-turbo\"\n",
    "TEMPERATURE = 0\n",
    "OPENAI_API_KEY = \"\"\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "#######################\n",
    "STYLE_FILE = \"data/conf_styles.pickle\"\n",
    "\n",
    "STYLE_PROMPT = \"\"\"\n",
    "당신은 현대 언어를 공자처럼 바꿔주는 전문 번역가입니다. \n",
    "다음 절차를 따라 저의 말을 번역해주세요.\n",
    "\n",
    "첫째, 실제 공자가 이야기하는 듯한 느낌을 주기 위하여 1인칭으로 바꾸어 말합니다. \n",
    "둘째, 공자가 말하듯이 공자의 말을 인용한다는 부분 또는 논어를 참고한다는 부분을 삭제합니다. \n",
    "셋째, 하오체를 사용합니다. \n",
    "\"\"\"\n",
    "#######################\n",
    "DATA_FILE = \"data/analects_raw.txt\"\n",
    "SPLIT_CHUNK_SIZE = 500\n",
    "SPLIT_CHUNK_OVERLAP = 10\n",
    "\n",
    "CONTEXT_PROMPT = \"\"\"\n",
    "당신은 'conf'를 바탕으로 답변을 만드는 챗봇입니다.\n",
    "다음 절차를 따라 저의 질문에 답변해 주세요.\n",
    "\n",
    "첫째, 질문에 답변하기 위해 'conf'에서 근거를 찾아 '근거:' 뒤에 출력하세요. 사용자의 질문과 조금이라도 관련되었다면 모두 근거로 사용하세요.\n",
    "만약 'conf'에 근거가 없다면 '없음'이라고 적고, 공자가 했을 뻡한 말을 답변: 뒤에 출력하세요.\n",
    "둘째, 근거가 있다면 해당 근거를 바탕으로 '답변:' 뒤에 답변을 출력하세요.\n",
    "\n",
    "근거: 여기에 근거를 출력\n",
    "답변: 여기에 답변을 출력\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Style\n",
    "import os\n",
    "from langchain.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "import pickle\n",
    "\n",
    "def style_init() -> LLMChain:\n",
    "    example_prompt = PromptTemplate(\n",
    "        input_variables=[\"input\", \"output\"],\n",
    "        template=\"Input: {input}\\nOutput: {output}\",\n",
    "    )\n",
    "    with open(STYLE_FILE, \"rb\") as pr:\n",
    "        examples = pickle.load(pr)\n",
    "\n",
    "    example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "        examples, OpenAIEmbeddings(), Chroma, k=10\n",
    "    )\n",
    "    similar_prompt = FewShotPromptTemplate(\n",
    "        example_selector=example_selector,\n",
    "        example_prompt=example_prompt,\n",
    "        prefix= STYLE_PROMPT,\n",
    "        suffix=\"Input: {input_noun}\\n\",\n",
    "        input_variables=[\"input_noun\"],\n",
    "    )\n",
    "    chat_model = ChatOpenAI(temperature=TEMPERATURE, model=MODEL_NAME)\n",
    "    chain = LLMChain(llm=chat_model, prompt=similar_prompt, verbose=True)\n",
    "    return chain\n",
    "\n",
    "style_chain = style_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3-1 Only GPT\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "def context_init() -> LLMChain:\n",
    "    prompt = PromptTemplate(input_variables=[\"in\"],\n",
    "    template=\"\"\"\n",
    "    너는 공자야.\n",
    "    input:{in}\n",
    "    \"\"\")\n",
    "    chat_model = ChatOpenAI(temperature=TEMPERATURE, model=MODEL_NAME)\n",
    "    chain = LLMChain(llm=chat_model,prompt=prompt, verbose=True)\n",
    "    return chain\n",
    "\n",
    "context_chain = context_init()\n",
    "num = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3-2 Data (ChatVectorDBChain)\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import ChatVectorDBChain\n",
    "\n",
    "def context_init() -> ChatVectorDBChain:\n",
    "    loader = TextLoader(DATA_FILE, encoding='utf-8')\n",
    "    data = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\\n\\n\",\n",
    "        chunk_size=SPLIT_CHUNK_SIZE, \n",
    "        chunk_overlap=SPLIT_CHUNK_OVERLAP\n",
    "    )\n",
    "    splits = text_splitter.split_documents(data)\n",
    "    \n",
    "    #vectordb\n",
    "    embedding = HuggingFaceEmbeddings()\n",
    "    vectordb = FAISS.from_documents(\n",
    "        documents = splits,\n",
    "        embedding = embedding\n",
    "    )\n",
    "    \n",
    "    model=ChatOpenAI(temperature=TEMPERATURE,model_name=MODEL_NAME)\n",
    "    chain=ChatVectorDBChain.from_llm(model,vectordb,return_source_documents=True, verbose=True)\n",
    "    return chain\n",
    "\n",
    "context_chain = context_init()\n",
    "num = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3-3 Data + Prompt (agent)\n",
    "from langchain.tools import Tool\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "def context_init() -> AgentExecutor:\n",
    "    # RetrievalQA\n",
    "    loader = TextLoader(DATA_FILE, encoding='utf-8')\n",
    "    data = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\\n\\n\",\n",
    "        chunk_size=SPLIT_CHUNK_SIZE, \n",
    "        chunk_overlap=SPLIT_CHUNK_OVERLAP\n",
    "    )\n",
    "    splits = text_splitter.split_documents(data)\n",
    "    \n",
    "    #vectordb\n",
    "    embedding = HuggingFaceEmbeddings()\n",
    "    vectordb = FAISS.from_documents(\n",
    "        documents = splits,\n",
    "        embedding = embedding\n",
    "    )\n",
    "    llm = ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE)\n",
    "    retrievalQA_chain = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff', retriever=vectordb.as_retriever())\n",
    "    personal_data_tool = Tool(name='conf', func=retrievalQA_chain.run, description='논어 데이터')\n",
    "    # agent\n",
    "    memory_key = 'history'\n",
    "    memory = ConversationBufferMemory(memory_key=memory_key, return_messages=True)\n",
    "    system_message = SystemMessage(content=CONTEXT_PROMPT)\n",
    "    prompt = OpenAIFunctionsAgent.create_prompt(system_message=system_message, extra_prompt_messages=[MessagesPlaceholder(variable_name=memory_key)])\n",
    "    agent = OpenAIFunctionsAgent(llm=llm, tools=[personal_data_tool], prompt=prompt)\n",
    "    agent_executor = AgentExecutor(agent=agent, tools=[personal_data_tool], memory=memory, verbose=True, handle_parsing_errors=True)\n",
    "    return agent_executor\n",
    "\n",
    "context_chain = context_init()\n",
    "num = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 Main\n",
    "def get_user_input():\n",
    "    return input('\\033[31m\\r\\nPrompt: \\033[0m')\n",
    "\n",
    "def main():\n",
    "    user_input = get_user_input()\n",
    "    if num == 1:\n",
    "        result=context_chain(inputs=user_input)\n",
    "        conf_result = style_chain(inputs=result['text'])\n",
    "    elif num == 2:\n",
    "        result=context_chain({'question':user_input,'chat_history':[]})\n",
    "        conf_result = style_chain(inputs=result['answer'])\n",
    "    elif num == 3:\n",
    "        result = context_chain({'input': user_input}, return_only_outputs=True)\n",
    "        conf_result = style_chain(inputs=result['output'])\n",
    "    \n",
    "    print(conf_result[\"text\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
